{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Importing gen_data.py\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import numpy.random as rn\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy.linalg import eigh, norm\n",
    "import sys\n",
    "import time\n",
    "import torch as tr\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import gen_data_package as gen_data_package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    # meta\n",
    "    'save_losses' : False,\n",
    "\n",
    "    # dataset\n",
    "    'dim' : 2,\n",
    "    'gen_x_func' : 'gen_x_circle_regular',\n",
    "    'gen_y_func' : 'gen_y_fourier_norm_1',\n",
    "    'add_phase' : False,\n",
    "    'n_val' : 1001,\n",
    "    'n_train' : 1001,\n",
    "    'ks' : [4, 8, 16, 32, 64, 128],\n",
    "    'resample' : False,\n",
    "\n",
    "    # network\n",
    "    'n_hidden' : 3,\n",
    "    'n_units' : 128,\n",
    "    #'kappa' : .05,\n",
    "    'hidden_bias' : 'zeros', # none/zeros/normal/default\n",
    "    'outer_fixed' : False,\n",
    "    'even_only' : False,\n",
    "    'odd_only' : False,\n",
    "\n",
    "    # optimization\n",
    "    'eta' : .001,\n",
    "    'n_epochs_max' : 20000,\n",
    "    'n_batch' : 0,\n",
    "    'stop_threshold_percent' : 1,\n",
    "    'max_training_time_in_minutes' : 600,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim(cfg, snapshot_epochs):\n",
    "    \n",
    "    if cfg['n_batch'] == 0: cfg['n_batch'] = cfg['n_train']\n",
    "\n",
    "    device = tr.device(\"cuda:0\" if tr.cuda.is_available() else \"cpu\")\n",
    "    use_parallel_gpus = False\n",
    "    print('\\r\\ndevice is: %s\\r\\n' % device)\n",
    "\n",
    "    ##################################################################################\n",
    "\n",
    "    # define network\n",
    "\n",
    "    class Net(nn.Module):\n",
    "\n",
    "        def __init__(self, d, n_sizes, hidden_bias, outer_fixed, out_size = 1):\n",
    "            super(Net, self).__init__()\n",
    "\n",
    "            n_hidden = len(n_sizes)\n",
    "            hidden_in_sizes = n_sizes[:]\n",
    "            hidden_in_sizes.insert(0,d)\n",
    "            hidden_out_sizes = n_sizes\n",
    "            outer_in_size = n_sizes[-1]\n",
    "            outer_out_size = out_size\n",
    "\n",
    "            self.hidden = nn.ModuleList()\n",
    "            self.hidden.extend([nn.Linear(hidden_in_sizes[i], hidden_out_sizes[i], bias=(hidden_bias!='none')) for i in range(n_hidden)])\n",
    "            for i in range(n_hidden):\n",
    "                # init normal weights in hidden layers\n",
    "                # tr.nn.init.normal_(self.hidden[i].weight, mean=0, std=kappa)\n",
    "                tr.nn.init.kaiming_normal_(self.hidden[i].weight, a=np.sqrt(5))\n",
    "\n",
    "                # init either 0's or normal biases in hidden layers\n",
    "                if hidden_bias == 'zeros':\n",
    "                    self.hidden[i].bias.data = tr.zeros([hidden_out_sizes[i]])\n",
    "                elif hidden_bias == 'normal':\n",
    "                    std = 1 / np.sqrt(3*hidden_in_sizes[i])\n",
    "                    tr.nn.init.normal_(self.hidden[i].bias, mean=0, std=std)\n",
    "            \n",
    "            self.act = nn.Tanh()\n",
    "            \n",
    "            if outer_fixed:\n",
    "                # set requires_grad to False, and initialize with values of +-1\n",
    "                self.outer = nn.Linear(outer_in_size, outer_out_size, bias=False)\n",
    "                for param in self.outer.parameters():\n",
    "                    param.requires_grad = False\n",
    "                self.outer.weight.data = tr.from_numpy(np.sign(rn.uniform(-1, 1, [outer_out_size, outer_in_size])) / np.sqrt(outer_in_size)).float()\n",
    "            else:\n",
    "                # initialize with normal\n",
    "                self.outer = nn.Linear(outer_in_size, outer_out_size, bias=False)\n",
    "                tr.nn.init.kaiming_normal_(self.outer.weight, a=np.sqrt(5))\n",
    "\n",
    "        def forward(self, x):\n",
    "            for i in range(len(self.hidden)):\n",
    "                x = self.act(self.hidden[i](x))\n",
    "            x = self.outer(x)\n",
    "            return x\n",
    "\n",
    "    class XYDataset():\n",
    "        def __init__(self, data):\n",
    "            self.x = tr.from_numpy(data['x']).float().view(len(data['x']), -1)\n",
    "            self.y = tr.from_numpy(data['y']).float().view(len(data['y']), -1)\n",
    "            self.vals = data['vals']\n",
    "            self.theta = tr.from_numpy(data['theta']).float().view(len(data['x']), -1)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.y)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.x[idx], self.y[idx], self.theta[idx]\n",
    "\n",
    "    def gen_data(cfg, pr=False):\n",
    "        data_train = gen_data_package.gen_xy(cfg, 'train')\n",
    "        if cfg['gen_y_func'] == 'gen_y_H_inf' or cfg['gen_y_func'] == 'gen_y_H_0' or cfg['gen_y_func'] == 'gen_y_H_inf_norm_1':\n",
    "            inds = rn.permutation(cfg['n_train'])\n",
    "            inds = inds[:cfg['n_train']]\n",
    "            data_val = {'x' : data_train['x'][inds, ...], 'y' : data_train['y'][inds], 'vals' : data_train['vals'], 'theta' : data_train['theta'][inds]}\n",
    "        else:\n",
    "            data_val = gen_data_package.gen_xy(cfg, 'val')\n",
    "\n",
    "        if pr:\n",
    "            plt.gray()\n",
    "            plt.scatter(data_train['x'][:,0], data_train['x'][:,1], c=data_train['y'])\n",
    "            \n",
    "        trainset = XYDataset(data_train)\n",
    "        valset = XYDataset(data_val)\n",
    "\n",
    "        return {'trainset' : trainset, 'valset' : valset, 'W' : data_train['W']}\n",
    "\n",
    "    def gen_net(cfg, W, device, use_parallel_gpus = False, pr = False):\n",
    "        net = Net(d=cfg['dim'],\n",
    "                    n_sizes=[cfg['n_units'] for i in range(cfg['n_hidden'])],\n",
    "                    hidden_bias=cfg['hidden_bias'],\n",
    "                    outer_fixed=cfg['outer_fixed'])\n",
    "        if pr: print('\\r\\nnet:\\r\\n', net)\n",
    "        if tr.cuda.device_count() > 1 and use_parallel_gpus:\n",
    "            print(\"Let's use\", tr.cuda.device_count(), \"GPUs!\")\n",
    "            net = nn.DataParallel(net)\n",
    "        net.to(device)\n",
    "        return net\n",
    "\n",
    "    def compute_lambda_0(trainset):\n",
    "        x = trainset.x.cpu().numpy()\n",
    "        gram = np.clip(np.matmul(x, x.T), -1, 1)\n",
    "        arcs = np.arccos(gram)\n",
    "        H = gram * (np.pi - arcs) / (2*np.pi)\n",
    "        vals, eigs = eigh(H, eigvals = (0, 0))\n",
    "        lambda_0 = vals[0]\n",
    "        return lambda_0\n",
    "\n",
    "    def optimize(cfg, device, net, datasets, pr = False):\n",
    "        \n",
    "        # gen data\n",
    "        trainset = datasets['trainset']\n",
    "        valset = datasets['valset']\n",
    "        \n",
    "        # set live plot:\n",
    "        fig = plt.figure()\n",
    "        ax_loss = fig.add_subplot(121)\n",
    "        ax_y = fig.add_subplot(122)\n",
    "        plt.ion()\n",
    "        fig.show()\n",
    "        fig.canvas.draw()\n",
    "        theta_val = valset.theta.cpu().numpy().reshape(-1).copy()\n",
    "        val_sort_inds = np.argsort(theta_val)\n",
    "        theta_val_sorted = valset.theta.cpu().numpy()[val_sort_inds].reshape(-1)\n",
    "        y_val_sorted = valset.y.cpu().numpy()[val_sort_inds].reshape(-1)\n",
    "        theta_train = trainset.theta.cpu().numpy().reshape(-1).copy()\n",
    "        train_sort_inds = np.argsort(theta_train)\n",
    "        theta_train_sorted = trainset.theta.cpu().numpy()[train_sort_inds].reshape(-1)\n",
    "        y_train_sorted = trainset.y.cpu().numpy()[train_sort_inds].reshape(-1)\n",
    "        \n",
    "        # compute eta\n",
    "        lambda_0 = compute_lambda_0(trainset)\n",
    "        print('\\r\\nlambda_0(H_\\\\infty) is: %g' % lambda_0)\n",
    "        eta = lambda_0 / cfg['n_train']**2\n",
    "        print('\\r\\nThreshold for learning rate is: %g' % eta)\n",
    "        print('\\r\\nConfigured learning rate is: %g' % cfg['eta'])\n",
    "\n",
    "        # create optimizer\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(net.parameters(), lr = cfg['eta'])\n",
    "\n",
    "        epoch_loss = np.zeros(cfg['n_epochs_max'])\n",
    "        train_loss = np.zeros(cfg['n_epochs_max'])\n",
    "        val_loss = np.zeros(cfg['n_epochs_max'])\n",
    "\n",
    "        with tr.no_grad():\n",
    "            val_output_before_training = net(valset.x.to(device))\n",
    "            val_loss_before_training = criterion(val_output_before_training, valset.y.to(device)) * cfg['n_val'] / 2\n",
    "            train_output_before_training = net(trainset.x.to(device))\n",
    "            train_loss_before_training = criterion(train_output_before_training, trainset.y.to(device)) * cfg['n_train'] / 2\n",
    "        print('\\r\\ninitial val/train loss is %.2f/%.2f\\r\\n' % (val_loss_before_training, train_loss_before_training))\n",
    "        \n",
    "        snapshot_train_outputs = []#[train_output_before_training.cpu().numpy()]\n",
    "\n",
    "        # train\n",
    "        converged = False\n",
    "        stopping_criterion = ''\n",
    "        n_batches = cfg['n_train'] // cfg['n_batch']\n",
    "        total_training_time = time.time()\n",
    "        total_optimization_time = 0\n",
    "        total_iter_time = 0\n",
    "        total_sampling_time = 0\n",
    "        total_compute_losses_time = 0\n",
    "        other_time = 0\n",
    "        epoch = 0\n",
    "        \n",
    "        while True:\n",
    "\n",
    "            total_sampling_time -= time.time()\n",
    "            if cfg['resample']:\n",
    "                epoch_datasets = gen_data(cfg, pr=False)\n",
    "                trainset = epoch_datasets['trainset']\n",
    "                #trainloader = epoch_datasets['trainloader']\n",
    "            total_sampling_time += time.time()\n",
    "\n",
    "            total_iter_time -= time.time()\n",
    "            epoch_Is = rn.permutation(cfg['n_train'])\n",
    "            total_iter_time += time.time()\n",
    "            accumulated_loss = .0\n",
    "\n",
    "            if epoch in snapshot_epochs:\n",
    "                with tr.no_grad():\n",
    "                    train_output_before = net(trainset.x.to(device))\n",
    "                print('epoch = %d, saving snapshot' % epoch)\n",
    "                snapshot_train_outputs.append(train_output_before.cpu().numpy())\n",
    "\n",
    "    #        for i, data in enumerate(trainloader):\n",
    "            for i_batch in range(n_batches):\n",
    "                # get the inputs\n",
    "                total_iter_time -= time.time()\n",
    "                batch_Is = epoch_Is[i_batch*cfg['n_batch']:(i_batch+1)*cfg['n_batch']]\n",
    "                x_train_batch, y_train_batch = trainset.x[batch_Is], trainset.y[batch_Is]\n",
    "                x_train_batch, y_train_batch = x_train_batch.to(device), y_train_batch.to(device)\n",
    "                total_iter_time += time.time()\n",
    "\n",
    "                total_optimization_time -= time.time()\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()   # zero the gradient buffers\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                output = net(x_train_batch)\n",
    "                loss = criterion(output, y_train_batch) * cfg['n_batch'] / 2\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_optimization_time += time.time()\n",
    "\n",
    "                # print statistics\n",
    "                accumulated_loss += loss.item()\n",
    "\n",
    "            total_compute_losses_time -= time.time()\n",
    "            epoch_loss[epoch] = accumulated_loss / n_batches\n",
    "            with tr.no_grad():\n",
    "                val_output = net(valset.x.to(device))\n",
    "                val_loss[epoch] = criterion(val_output, valset.y.to(device)) * cfg['n_val'] / 2\n",
    "                train_output_after = net(trainset.x.to(device))\n",
    "                train_loss[epoch] = criterion(train_output_after, trainset.y.to(device)) * cfg['n_train'] / 2\n",
    "            total_compute_losses_time += time.time()\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                \n",
    "                ax_loss.clear()\n",
    "                ax_loss.plot(train_loss[min(50,epoch):epoch], '.-')\n",
    "                ax_loss.set_xlabel('epochs')\n",
    "                ax_loss.set_ylabel('train MSE')\n",
    "                \n",
    "                ax_y.clear()\n",
    "                ax_y.plot(theta_train_sorted, y_train_sorted, '--y', label='target on train set')\n",
    "                ax_y.plot(theta_val_sorted, val_output.cpu().numpy().reshape(-1)[val_sort_inds], '-', label='output on val set')\n",
    "                ax_y.plot(theta_train_sorted, train_output_before.cpu().numpy().reshape(-1)[train_sort_inds], '.', label='output on train set')\n",
    "                ax_y.set_ylim([2*np.min(y_val_sorted), 2*np.max(y_val_sorted)])\n",
    "                ax_y.set_xlabel('x')\n",
    "                ax_y.set_ylabel('label/output')\n",
    "                ax_y.set_title('k = %d, epoch = %d' % (cfg['ks'][0], epoch))\n",
    "                ax_y.legend()\n",
    "                \n",
    "                fig.canvas.draw()            \n",
    "\n",
    "            epoch += 1\n",
    "            if pr and epoch % 500 == 0:\n",
    "                print('epoch %7d val/train loss: %.2f/%.2f (%.2f%% for training)\\r\\n' %\n",
    "                      (epoch, val_loss[epoch-1], train_loss[epoch-1], train_loss[epoch-1] / cfg['zero_val_loss'] * 100))\n",
    "            #if train_loss[epoch-1] / cfg['zero_val_loss'] < cfg['stop_threshold_percent'] / 100:\n",
    "            if train_loss[epoch-1] < cfg['stop_threshold_percent'] / 100:\n",
    "                print('\\r\\n@@@ after %d epochs, training loss is %.2f, reached stop threshold of %.2f%% and training is done\\r\\n'\n",
    "                      % (epoch, train_loss[epoch-1], cfg['stop_threshold_percent']))\n",
    "                converged = True\n",
    "                stopping_criterion = 'converged'\n",
    "                break\n",
    "            training_time_so_far_in_minutes = (time.time() - total_training_time) / 60\n",
    "            if training_time_so_far_in_minutes > cfg['max_training_time_in_minutes']:\n",
    "                print('\\r\\nafter %d epochs, training time exceeded %d minutes threshold and training is stopped\\r\\n'\n",
    "                      % (epoch, cfg['max_training_time_in_minutes']))\n",
    "                stopping_criterion = 'time_out'\n",
    "                break\n",
    "            if np.isnan(epoch_loss[epoch-1]):\n",
    "                print('\\r\\nafter %d epochs, epoch loss is NaN and training is stopped\\r\\n'\n",
    "                     % epoch)\n",
    "                stopping_criterion = 'reached_nan'\n",
    "                break\n",
    "            if epoch == cfg['n_epochs_max']:\n",
    "                print('\\r\\nreached maximal number of epochs %d and training is stopped\\r\\n'\n",
    "                     % cfg['n_epochs_max'])\n",
    "                stopping_criterion = 'epochs_over'\n",
    "                break\n",
    "\n",
    "        #total_compute_losses_time += time.time()\n",
    "        total_training_time = time.time() - total_training_time\n",
    "        print('\\r\\nfinished training! training time: %.2f minutes (optimization time: %.2f%%, iter time: %.2f%%, compute train/val loss time: %.2f%%, sampling time: %.2f%%\\r\\n)'\n",
    "              % (total_training_time/60,\n",
    "              total_optimization_time/total_training_time*100,\n",
    "              total_iter_time/total_training_time*100,\n",
    "              total_compute_losses_time/total_training_time*100,\n",
    "              total_sampling_time/total_training_time*100))\n",
    "\n",
    "        with tr.no_grad():\n",
    "            val_output_after_training = net(valset.x.to(device))\n",
    "            val_loss_after_training = criterion(val_output_after_training, valset.y.to(device)) * cfg['n_val'] / 2\n",
    "            train_output_after_training = net(trainset.x.to(device))\n",
    "            train_loss_after_training = criterion(train_output_after_training, trainset.y.to(device)) * cfg['n_train'] / 2\n",
    "        print('\\r\\nfinal val/train loss is %.2f/%.2f\\r\\n (%.2f%% for validation)' % (val_loss_after_training, train_loss_after_training,\n",
    "            val_loss_after_training / cfg['zero_val_loss'] * 100))\n",
    "        snapshot_epochs.append(epoch)\n",
    "        snapshot_train_outputs.append(train_output_after_training.cpu().numpy())\n",
    "    \n",
    "        return {'num_epochs' : epoch,\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'final_train_loss' : train_loss_after_training.cpu().numpy().flatten()[0],\n",
    "                'final_val_loss' : val_loss_after_training.cpu().numpy().flatten()[0],\n",
    "                'converged' : converged,\n",
    "                'training_time' : total_training_time,\n",
    "                'stopping_criterion' : stopping_criterion,\n",
    "                'lambda_0' : lambda_0,\n",
    "                'theta_train' : trainset.theta.cpu().numpy(),\n",
    "                'y_train' : trainset.y.cpu().numpy(),\n",
    "                'snapshot_epochs' : np.array(snapshot_epochs),\n",
    "                'snapshot_train_outputs' : np.array(snapshot_train_outputs),\n",
    "               }\n",
    "\n",
    "    results = pd.DataFrame()\n",
    "\n",
    "    if cfg['add_phase']:\n",
    "        cfg['phases'] = rn.uniform(-np.pi, np.pi, len(cfg['ks']))\n",
    "    else:\n",
    "        cfg['phases'] = []\n",
    "    print('\\r\\ntest cfg: \\r\\n', cfg, '\\r\\n')\n",
    "\n",
    "    total_loops_time = time.time()\n",
    "\n",
    "    pr = True\n",
    "\n",
    "    # generate new dataset\n",
    "    datasets = gen_data(cfg, pr=False)\n",
    "    cfg['zero_val_loss'] = tr.sum(datasets['valset'].y**2).cpu().cpu().numpy() / 2\n",
    "    if pr: print('zero val loss is: %.3f\\r\\n' % cfg['zero_val_loss'])\n",
    "\n",
    "    # create new network\n",
    "    net = gen_net(cfg, datasets['W'], device, use_parallel_gpus, pr)\n",
    "\n",
    "    # optimize and save results\n",
    "    my_dict = {'ks' : cfg['ks'], 'phases' : cfg['phases'], 'eigenvalues' : datasets['trainset'].vals}\n",
    "    my_dict.update(optimize(cfg, device, net, datasets, pr))\n",
    "    results = results.append(my_dict, ignore_index = True)\n",
    "\n",
    "    total_loops_time = (time.time() - total_loops_time) / 60\n",
    "    print('\\r\\nfinished looping! total looping time: %.2f minutes\\r\\n' % total_loops_time)\n",
    "\n",
    "    print(results)\n",
    "    print('\\r\\nstopping criterion: %s' % results['stopping_criterion'])\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the sim\n",
    "results = sim(cfg, [0, 100, 1000, 10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_pickle('results/sin_waves_nn.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
